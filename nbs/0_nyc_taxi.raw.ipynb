{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous profile: cloudbend\n",
      "Setting new profile to: cloudbend\n",
      "Setting Glue version to: 3.0\n",
      "Current iam_role is arn:aws:iam::898546127587:role/GlueSessions\n",
      "iam_role has been set to arn:aws:iam::898546127587:role/GlueSessions.\n",
      "Current idle_timeout is 60 minutes.\n",
      "idle_timeout has been set to 60 minutes.\n",
      "Previous worker type: G.1X\n",
      "Setting new worker type to: G.1X\n",
      "Previous number of workers: 5\n",
      "Setting new number of workers to: 10\n",
      "Additional python modules to be included:\n",
      "tqdm\n"
     ]
    }
   ],
   "source": [
    "%profile \"cloudbend\"\n",
    "\n",
    "%glue_version \"3.0\"\n",
    "%iam_role \"arn:aws:iam::898546127587:role/GlueSessions\"\n",
    "%idle_timeout 60\n",
    "\n",
    "%worker_type \"G.1X\"\n",
    "%number_of_workers 10\n",
    "\n",
    "%additional_python_modules tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with profile=cloudbend\n",
      "glue_role_arn defined by user: arn:aws:iam::898546127587:role/GlueSessions\n",
      "Trying to create a Glue session for the kernel.\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 10\n",
      "Session ID: f0f45170-a03b-4b13-a8fa-8c691624be1c\n",
      "Job Type: glueetl\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 0.38.1\n",
      "--enable-glue-datacatalog true\n",
      "--additional-python-modules tqdm\n",
      "Waiting for session f0f45170-a03b-4b13-a8fa-8c691624be1c to get into ready status...\n",
      "Session f0f45170-a03b-4b13-a8fa-8c691624be1c has been created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# list all files\n",
    "response = s3.list_objects_v2(Bucket=\"nyc-tlc\", Prefix=\"trip data/\")\n",
    "paths = [f\"s3://nyc-tlc/{x['Key']}\" for x in response[\"Contents\"]]\n",
    "\n",
    "# only include parquet files\n",
    "paths = [x for x in paths if x.endswith(\".parquet\")]\n",
    "\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "bucket = \"pyicebergbenchmark-icebergbucket89dd3fa6-1qhkzajoxgpzo\"\n",
    "\n",
    "column_dtypes = {\n",
    "    \"PUlocationID\": \"string\",\n",
    "    \"DOlocationID\": \"string\",\n",
    "    \"trip_type\": \"string\",\n",
    "    \"rate_code\": \"string\",\n",
    "    \"improvement_surcharge\": \"double\",\n",
    "    \"congestion_surcharge\": \"double\",\n",
    "    \"ehail_fee\": \"double\",\n",
    "    \"VendorID\": \"string\",\n",
    "    \"SR_Flag\": \"string\",\n",
    "    \"airport_fee\": \"double\",\n",
    "    \"payment_type\": \"string\",\n",
    "}\n",
    "\n",
    "def standardize_fragment(path: str, prefix: str) -> int:\n",
    "    df = spark.read.parquet(path)\n",
    "\n",
    "    for column, dtype in column_dtypes.items():\n",
    "        if column in df.columns:\n",
    "            df = df.withColumn(column, col(column).cast(dtype))\n",
    "        else:\n",
    "            df = df.withColumn(column, lit(None).cast(dtype))\n",
    "\n",
    "    total = df.count()\n",
    "\n",
    "    df.write.mode(\"overwrite\").parquet(f\"s3://{bucket}/raw/{prefix}/\")\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3318228997\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    totals = executor.map(\n",
    "        lambda args: standardize_fragment(*args),\n",
    "        [(path, str(i)) for i, path in enumerate(paths)]\n",
    "    )\n",
    "\n",
    "    sum(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Py4JJavaError: An error occurred while calling o21629.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 2158.0 failed 4 times, most recent failure: Lost task 16.3 in stage 2158.0 (TID 18098) (172.35.220.134 executor 1): org.apache.spark.SparkException: Failed merging schema:\n",
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: string (nullable = true)\n",
      " |-- DOlocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      "\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:86)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4$adapted(SchemaMergeUtils.scala:81)\n",
      "\tat scala.collection.immutable.Stream.foreach(Stream.scala:529)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Failed to merge fields 'pickup_datetime' and 'pickup_datetime'. Failed to merge incompatible data types string and timestamp\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:590)\n",
      "\tat scala.Option.map(Option.scala:146)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:582)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1$adapted(StructType.scala:579)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:32)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:194)\n",
      "\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:579)\n",
      "\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:489)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:83)\n",
      "\t... 16 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2465)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2414)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2413)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2413)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2679)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2621)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2610)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:926)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:107)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:174)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n",
      "\tat scala.Option.orElse(Option.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:418)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:834)\n",
      "\tat sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Failed merging schema:\n",
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: string (nullable = true)\n",
      " |-- DOlocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      "\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:86)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4$adapted(SchemaMergeUtils.scala:81)\n",
      "\tat scala.collection.immutable.Stream.foreach(Stream.scala:529)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Failed to merge fields 'pickup_datetime' and 'pickup_datetime'. Failed to merge incompatible data types string and timestamp\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:590)\n",
      "\tat scala.Option.map(Option.scala:146)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:582)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1$adapted(StructType.scala:579)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:32)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:194)\n",
      "\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:579)\n",
      "\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:489)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:83)\n",
      "\t... 16 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.read\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .parquet(f\"s3://{bucket}/raw/*/\")\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameError: name 'df' is not defined\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping session: 35b91180-4f42-452d-95de-b6fcfce2734a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped session.\n"
     ]
    }
   ],
   "source": [
    "%stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
