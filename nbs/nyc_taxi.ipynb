{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous profile: None\n",
      "Setting new profile to: cloudbend\n",
      "Setting Glue version to: 3.0\n",
      "Current iam_role is None\n",
      "iam_role has been set to arn:aws:iam::898546127587:role/GlueSessions.\n",
      "Current idle_timeout is 2880 minutes.\n",
      "idle_timeout has been set to 60 minutes.\n",
      "Previous worker type: G.1X\n",
      "Setting new worker type to: G.1X\n",
      "Previous number of workers: 5\n",
      "Setting new number of workers to: 5\n",
      "Extra jars to be included:\n",
      "s3://lambdaicebergdemo-icebergbucket89dd3fa6-fyq2q519smob/jars/iceberg/iceberg-spark-runtime-3.0_2.12-1.0.0.jar\n",
      "s3://lambdaicebergdemo-icebergbucket89dd3fa6-fyq2q519smob/jars/aws/bundle-2.17.257.jar\n",
      "s3://lambdaicebergdemo-icebergbucket89dd3fa6-fyq2q519smob/jars/aws/url-connection-client-2.17.257.jar\n",
      "s3://lambdaicebergdemo-icebergbucket89dd3fa6-fyq2q519smob/jars/iceberg/iceberg-spark-runtime-3.0_2.12-1.0.0.jar,s3://lambdaicebergdemo-icebergbucket89dd3fa6-fyq2q519smob/jars/aws/bundle-2.17.257.jar,s3://lambdaicebergdemo-icebergbucket89dd3fa6-fyq2q519smob/jars/aws/url-connection-client-2.17.257.jar\n"
     ]
    }
   ],
   "source": [
    "%profile \"cloudbend\"\n",
    "\n",
    "%glue_version \"3.0\"\n",
    "%iam_role \"arn:aws:iam::898546127587:role/GlueSessions\"\n",
    "%idle_timeout 60\n",
    "\n",
    "%worker_type \"G.1X\"\n",
    "%number_of_workers 5\n",
    "\n",
    "%extra_jars \"s3://lambdaicebergdemo-icebergbucket89dd3fa6-fyq2q519smob/jars/iceberg/iceberg-spark-runtime-3.0_2.12-1.0.0.jar,s3://lambdaicebergdemo-icebergbucket89dd3fa6-fyq2q519smob/jars/aws/bundle-2.17.257.jar,s3://lambdaicebergdemo-icebergbucket89dd3fa6-fyq2q519smob/jars/aws/url-connection-client-2.17.257.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are already connected to a glueetl session bfefe1ee-8362-4b3c-b00e-fecd888a7466.\n",
      "\n",
      "No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following configurations have been updated: {'conf': 'spark.jars.packages=org.apache.iceberg:iceberg-spark-runtime-3.0_2.12-1.0.0,software.amazon.awssdk:bundle:2.17.257,software.amazon.awssdk:url-connection-client:2.17.257 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'}\n"
     ]
    }
   ],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"conf\": \"spark.jars.packages=org.apache.iceberg:iceberg-spark-runtime-3.0_2.12-1.0.0,software.amazon.awssdk:bundle:2.17.257,software.amazon.awssdk:url-connection-client:2.17.257 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "catalog = \"iceberg\"\n",
    "bucket = \"pyicebergbenchmark-icebergbucket89dd3fa6-1qhkzajoxgpzo\"\n",
    "prefix = \"iceberg\"\n",
    "\n",
    "iceberg_conf = [\n",
    "    (f\"spark.sql.catalog.{catalog}\", \"org.apache.iceberg.spark.SparkCatalog\"),\n",
    "    (f\"spark.sql.catalog.{catalog}.warehouse\", f\"s3://{bucket}/{prefix}/{catalog}/\"),\n",
    "    (f\"spark.sql.catalog.{catalog}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\"),\n",
    "    (f\"spark.sql.catalog.{catalog}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\"),\n",
    "]\n",
    "\n",
    "for k, v in iceberg_conf:\n",
    "    spark.conf.set(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnalysisException: cannot resolve '`trip_type`' given input columns: [Affiliated_base_number, DOlocationID, PUlocationID, SR_Flag, dispatching_base_num, dropOff_datetime, pickup_datetime];\n",
      "'Project [dispatching_base_num#278, pickup_datetime#279, dropOff_datetime#280, PUlocationID#281, DOlocationID#282, SR_Flag#283, Affiliated_base_number#284, cast('trip_type as string) AS trip_type#292]\n",
      "+- Relation[dispatching_base_num#278,pickup_datetime#279,dropOff_datetime#280,PUlocationID#281,DOlocationID#282,SR_Flag#283,Affiliated_base_number#284] parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# list all files\n",
    "response = s3.list_objects_v2(Bucket=\"nyc-tlc\", Prefix=\"trip data/\")\n",
    "paths = [f\"s3://nyc-tlc/{x['Key']}\" for x in response[\"Contents\"]]\n",
    "\n",
    "rdds = []\n",
    "for path in paths:\n",
    "    df = (\n",
    "        spark.read\n",
    "            .parquet(path)\n",
    "            .withColumn(\"trip_type\", col(\"trip_type\").cast(\"string\")) # bigints & doubles in source data\n",
    "            .withColumn(\"rate_code\", col(\"rate_code\").cast(\"string\")) # bigints & strings in source data\n",
    "            .withColumn(\"improvement_surcharge\", col(\"improvement_surcharge\").cast(\"double\")) # ints & doubles in source data\n",
    "            .withColumn(\"congestion_surcharge\", col(\"congestion_surcharge\").cast(\"double\")) # ints & doubles in source data\n",
    "            .withColumn(\"ehail_fee\", col(\"ehail_fee\").cast(\"double\")) # ints & doubles in source data\n",
    "            .withColumn(\"VendorID\", col(\"VendorID\").cast(\"string\")) # ints & bigints in source data\n",
    "            .withColumn(\"SR_Flag\", col(\"SR_Flag\").cast(\"string\")) # ints & doubles in source data\n",
    "            .withColumn(\"PUlocationID\", col(\"PUlocationID\").cast(\"string\")) # bigints & doubles in source data\n",
    "    )\n",
    "\n",
    "    rdds.append(df.rdd)\n",
    "\n",
    "df = spark.sparkContext.union(rdds).toDF()\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+----------------------+\n",
      "|              B00001|2017-06-01 00:04:00|2017-06-07 01:55:36|                      |\n",
      "|              B00001|2017-06-01 00:47:00|2017-06-09 01:25:40|                      |\n",
      "|              B00001|2017-06-01 00:07:00|2017-06-15 00:52:17|                      |\n",
      "|              B00001|2017-06-01 00:30:00|2017-06-03 02:18:52|                      |\n",
      "|              B00001|2017-06-01 00:45:00|2017-06-10 01:49:42|                      |\n",
      "|              B00001|2017-06-01 00:25:00|2017-06-19 00:58:48|                      |\n",
      "|              B00013|2017-06-01 00:30:19|2017-06-01 00:33:34|                B00013|\n",
      "|              B00013|2017-06-01 00:26:12|2017-06-02 00:27:45|                B00013|\n",
      "|              B00014|2017-06-01 00:54:03|2017-06-01 01:18:57|                B00014|\n",
      "|              B00014|2017-06-01 00:05:48|2017-06-02 00:06:12|                B00014|\n",
      "|              B00014|2017-06-01 00:26:01|2017-06-02 00:29:38|                B00014|\n",
      "|              B00014|2017-06-01 00:48:39|2017-06-01 00:48:47|                B00014|\n",
      "|              B00014|2017-06-01 00:12:27|2017-06-01 00:51:24|                B00014|\n",
      "|              B00014|2017-06-01 00:27:12|2017-06-01 00:27:26|                B00014|\n",
      "|              B00014|2017-06-01 00:55:40|2017-06-01 01:20:56|                B00014|\n",
      "|              B00014|2017-06-01 00:58:19|2017-06-01 01:04:04|                B00014|\n",
      "|              B00014|2017-06-01 00:30:55|2017-06-01 00:31:18|                B00014|\n",
      "|              B00021|2017-06-01 00:00:15|2017-06-01 00:05:42|                B00021|\n",
      "|              B00021|2017-06-01 00:59:05|2017-06-01 01:37:26|                B00021|\n",
      "|              B00021|2017-06-01 00:00:54|2017-06-01 00:07:00|                B00021|\n",
      "+--------------------+-------------------+-------------------+----------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"nyc_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|count(DISTINCT dispatching_base_num)|\n",
      "+------------------------------------+\n",
      "|                                1333|\n",
      "+------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(DISTINCT dispatching_base_num)\n",
    "FROM nyc_taxi;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS iceberg.benchmark;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Py4JJavaError: An error occurred while calling o69.sql.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 195 in stage 60.0 failed 4 times, most recent failure: Lost task 195.3 in stage 60.0 (TID 4458) (172.34.78.30 executor 3): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainBinaryDictionary\n",
      "\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n",
      "\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n",
      "\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)\n",
      "\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2465)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2414)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2413)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2413)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2679)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2621)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2610)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\n",
      "\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:309)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:197)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:191)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:167)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:163)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$2(ShuffleExchangeExec.scala:100)\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$1(ShuffleExchangeExec.scala:100)\n",
      "\tat org.apache.spark.sql.util.LazyValue.getOrInit(LazyValue.scala:41)\n",
      "\tat org.apache.spark.sql.execution.exchange.Exchange.getOrInitMaterializeFuture(Exchange.scala:68)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materializeFuture(ShuffleExchangeExec.scala:96)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize(ShuffleExchangeExec.scala:84)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize$(ShuffleExchangeExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.materialize(ShuffleExchangeExec.scala:128)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:161)\n",
      "\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:74)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n",
      "\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:74)\n",
      "\tat org.apache.spark.sql.execution.adaptive.MaterializeExecutable.tryStart(AdaptiveExecutable.scala:396)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.startChild(AdaptiveExecutor.scala:225)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper.start(ExecutionHelper.scala:47)\n",
      "\tat org.apache.spark.sql.execution.adaptive.QueryStageExecutable$$anon$2.$anonfun$new$1(AdaptiveExecutable.scala:251)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2(ExecutionHelper.scala:55)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2$adapted(ExecutionHelper.scala:54)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1(ExecutionHelper.scala:54)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1$adapted(ExecutionHelper.scala:53)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.onChildSuccess(ExecutionHelper.scala:53)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2(AdaptiveExecutor.scala:314)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2$adapted(AdaptiveExecutor.scala:314)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onActiveChildSuccess(AdaptiveExecutor.scala:314)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onChildSuccess(AdaptiveExecutor.scala:284)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1(AdaptiveExecutor.scala:92)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1$adapted(AdaptiveExecutor.scala:91)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:145)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:235)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:228)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:145)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:434)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.writeWithV2(WriteToDataSourceV2Exec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:476)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:465)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:118)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainBinaryDictionary\n",
      "\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n",
      "\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n",
      "\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)\n",
      "\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE iceberg.benchmark.nyc_taxi\n",
    "USING iceberg\n",
    "PARTITIONED BY (dispatching_base_num)\n",
    "TBLPROPERTIES (\n",
    "    'write.parquet.compression-codec' = 'snappy',\n",
    "    'write.metadata.metrics.default' = 'full'\n",
    ")\n",
    "AS\n",
    "SELECT *\n",
    "FROM nyc_taxi\n",
    "ORDER BY pickup_datetime;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping session: bfefe1ee-8362-4b3c-b00e-fecd888a7466\n",
      "Stopped session.\n"
     ]
    }
   ],
   "source": [
    "%stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
